\section{Overview of Apache Griffin}
Apache Griffin is an open source data quality solution for big data. It is described by its developers as a data quality services platform. As such, it provides a comprehensive analysis framework, allowing for various tasks such as defining a data quality model, calculating various metrics, automating data validation and providing a unified visualisation of metrics. Apache Griffin (Griffin), through its ecosystem of infrastructures, enables the management of both real-time (streaming) and batch data, while addressing the quality challenges in big data applications. \\

In order to meet different needs, Apache Griffin offers a number of features. As presented in the documentation, we have :
\begin{itemize}[parsep=0cm,itemsep=0cm]
    \item defining metrics: accuracy, completeness, timeliness, uniqueness, validity, consistency ;
    \item anomaly monitoring: setting specific expectations to detect abnormal data, while offering the possibility to download them;
    \item anomaly alerts: report data quality issues via email or on the platform;
    \item visual monitoring: thanks to its web interface, one can appreciate the state of data quality over time;
    \item real time: data quality inspection can be performed in real time;
    \item elasticity and agility:  Griffin can be used to analyse data from multiple data warehouses. Moreover, it operates in an environment (Spark and Hadoop) that can handle large volumes of data (1.2 Petabytes, in the case of eBay);
    \item self-service: Griffin provides a simple and easy-to-use user interface that can manage data quality rules; at the same time, users can visualise data quality results and customise the display content through the control panel.
\end{itemize}
