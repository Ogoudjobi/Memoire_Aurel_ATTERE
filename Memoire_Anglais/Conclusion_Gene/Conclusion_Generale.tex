\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{ CONCLUSION}

\vspace*{0.5cm}
\section*{\centering \Huge  Conclusion}
\titlerule[2.0pt]
\vspace{1cm}
The crucial problem of data quality at the \acrlong{df} led us to implement Apache Griffin, a data quality assessment tool, in order to be able to detect anomalies in the data base and subsequently propose correction algorithms. In order to carry out this task, we set out to understand the theoretical framework of quality analysis. This understanding has in turn helped us to understand the Apache Griffin tool, and to better understand its functionalities. \\

Faced with the \acrshort{df}'s quality and service requirements, we applied our technical knowledge to adapt the tool to the needs formulated. This adaptation allowed an efficient evaluation of the quality of the data in the database and helped to highlight several irregularities along the following dimensions: Completeness, Uniqueness, Validity and Consistency. The next stage of the project consisted in correcting the anomalies detected. This led us to write algorithms in PySpark to make irregular data reliable. The visualisation of metrics on the dashboard created for this purpose, allows decision-makers to follow the evolution of the quality of the different data over time. \\

The next step in the quality component of the data governance policy of the foundation must include the clear definition of metrics and thresholds as well as the integration of the tool into their architecture. Quality management must therefore start with a well-documented plan and strategy. This will enable the tools to be configured to assess quality. The results will help to measure the extent of data quality problems and determine whether the data acquisition and analysis process is adequate. Finally, the last component, quality improvement, is to clean up the data, understand the non-quality and take steps to improve the process. This will allow the plan to be adjusted and so on.