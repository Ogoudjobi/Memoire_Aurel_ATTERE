\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{INTRODUCTION}
\pagenumbering{arabic}
\vspace*{0.5cm}
\section*{\centering \Huge Introduction}
\titlerule[2.0pt]
\vspace{1cm}
 %Pour les entreprises, c'est un moyen d'augmenter significativement leur profit sur une période donnée. Ainsi, \`a travers les innovations techniques qu'elles induisent, les diff\'erentes r\'evolutions industrielles ont profondément  transformé l'économie. [...] Bien que déjà visible dans ses réalisations comme l’intelligence artificielle, la nanotechnologie ou l’information quantique, nous en sommes qu’au début.%
%En \'economie, le progrès technique est considéré comme une source majeure de croissance surtout pour les entreprises. Ainsi, 

The world is now facing the beginnings of a fourth industrial revolution driven by the rise of Big Data. The explosion in the volume of data, as well as the emergence and popularisation of new technologies for storing, processing and leveraging data, has led to profound changes within companies. In order to benefit from the externalities of this revolution, companies are increasingly turning to data-driven and data-centric approaches. Nowadays, companies perform big data analysis, diagnostic modelling and data processing to achieve market excellence. So it goes without saying that any company that wants to be data-driven and data-centric needs to have good data to rely on. Otherwise, you can imagine the disastrous decisions that could be made. It is therefore necessary, even essential for any company to have a solid data governance strategy and therefore data quality.\\

With this in mind, Saham Assurance Maroc, through its Digital Factory, has decided to implement a data governance policy as part of its data foundation project. This policy pays particular attention to the quality aspect. Indeed, data quality is very important for insurance companies, in that it greatly facilitates the insurer's activity in determining risks, calculating rates and even detecting fraud, without forgetting the regulatory requirements of the Solvency 2 prudential regime. Poor quality data can lead to a deterioration or lengthening of the work and the resulting analyses. It impacts the quality of services offered to clients. Indeed, according to a study by the \acrlong{mit} (\acrshort{mit}, 2017) \cite{MIT2017}, non-quality results in an estimated loss of money of between 15\% and 25\% of total revenue for most companies. About 20 years ago, this loss was estimated at 5\% - 10\% of company revenue \cite{Techno7}.\\

It is in this context that the following subject: detection and correction of data quality problems within the data of the \acrlong{df}, was entrusted to us during our six-month internship at the \acrlong{df} of Saham Assurance Maroc. The main objective of this study is to detect with Apache Griffin the quality problems of tables of the data base, then to apply, when possible, algorithms to correct the anomalies detected. To do this, we followed a methodology in three (3) main steps :
\begin{itemize}[parsep=0cm,itemsep=0cm]
    \item installation and configuration of Apache Griffin to meet the requirements of the \acrlong{df};
    \item the use of Apache Griffin to detect anomalies and inconsistencies in the data base and ;
    \item the proposal of correction algorithms
\end{itemize}

Before approaching and applying this methodology, the first chapter will situate the study in its contextual environment by presenting the company and the details of the project. The second chapter will provide a better understanding of the underlying theoretical considerations. In the third chapter we will present the main results obtained by applying the above methodology.


