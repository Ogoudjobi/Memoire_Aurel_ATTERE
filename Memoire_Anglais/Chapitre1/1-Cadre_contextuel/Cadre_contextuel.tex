%\section{Cadre contextuel}

\section{Presentation of the host company}
Founded in 1918 as a life insurance company, Sanlam (South African National Life Assurance Company Limited) is a leading diversified financial services group based in South Africa. It operates throughout the African continent as well as in Malaysia, the United States, France, Switzerland, India and Australia. It is one of the largest insurance groups with a direct and indirect presence in forty-five (45) countries. Within the Group, our internship took place at the Sanlam Pan-Africa pole, more precisely at the Digital Factory which is the the digital pole of the Moroccan subsidiary. Because of the sanitary conditions, our internship was carried out physically at Colina Participations but virtually (remotely) at the \acrshort{df}.


%\subsection{Data quality in insurance}
%Insurance is defined as an operation by which the insured transfers his risks to the insurer in exchange for the payment of a premium. It is distinguished by the inversion of the production cycle. Indeed, it is impossible for companies to know with certainty how much the service they sell will cost them, as the premium is paid by the client before the indemnity. Therefore, to set the premium or calculate the reserves, the insurer can only rely on statistical studies that give an idea of how much the service will cost by analyzing, for example, the loss ratio and the average cost of previous claims. The reliability of information systems and the quality of data must therefore be a permanent objective. The perfect control of information systems
%allows us to :
%\begin{itemize}[parsep=0cm,itemsep=0cm]
%    \item distinguish itself in terms of pricing ;
%    \item distinguish itself in terms of risk management;
%    \item set up better fraud detection systems.
%\end{itemize}
%As a result, data quality flaws can be an impediment to the group's competitiveness against competitors and can be costly for several reasons.

%First, they (data quality defects) make all production work more difficult because they make processing more complex. In addition, poor quality data of poor quality are likely to lead to a degradation or a lengthening of the work and the resulting analyses. Finally, they impact the quality of of services offered to clients. Indeed, according to a study by the Massachusetts Institute of Technology (MIT) (MIT, 2017) [1], non-quality causes a loss of money estimated to be between 15\% and 25\% of most companies' total revenue. About 20 years earlier, this loss was estimated to be 5\% - 10\% of company revenue [2]. Similarly, in [3], the Gartner Institute estimates that more than 25\% of the data of the world's largest largest companies in the world are in error and also states that such a problem consequence but rather a commercial consequence, amounting to millions of in millions of dollars. It is therefore urgent to take a more serious look at this problem while taking into account the technological context. Anchoring any data governance strategy governance strategy, the implementation of a data quality project requires a few questions: what is data quality and how can it be measured? how to measure it? What are the different data quality tools and what are the advantages of Apache Griffin? What are the advantages of Apache Griffin over other data quality tools? How do I fix the data quality data quality problems that are detected? 

